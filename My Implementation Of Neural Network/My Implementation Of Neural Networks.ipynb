{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs)\n",
    "Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How exactly do NN “learn” stuff?**\n",
    "\n",
    "In the same way that we learn from experience in our lives, neural networks require data to learn. In most cases, the more data that can be thrown at a neural network, the more accurate it will become. Think of it like any task you do over and over. Over time, you gradually get more efficient and make fewer mistakes.\n",
    "\n",
    "When researchers or computer scientists set out to train a neural network, they typically divide their data into three sets. First is a training set, which helps the network establish the various weights between its nodes. After this, they fine-tune it using a validation data set. Finally, they’ll use a test set to see if it can successfully turn the input into the desired output.\n",
    "\n",
    "**Do neural networks have any limitations?**\n",
    "\n",
    "Biggest challenge with neural networks is the significantly large training time and the amount of computation power required to train the neural network. The biggest issue, however, is that neural networks are “black boxes,” in which the user feeds in data and receives answers. They can fine-tune the answers, but they don’t have access to the exact decision making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4NN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivativeSig(sig_out):\n",
    "    return sig_out*(1 - sig_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1], \n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XOR decision Boundry**\n",
    "\n",
    "Non linear\n",
    "\n",
    "![title](5NN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60059059],\n",
       "       [ 0.05477104],\n",
       "       [ 0.93807049]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1       # generating random weights between -1 and 1 \n",
    "learning_rate = 0.1\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (3, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](7NN.jpg)\n",
    "Let's use the above mentioned error fuction for following unit:\n",
    "![title](6NN.jpg)\n",
    "\n",
    "here input has n features per training example , consequently n weights and 1 bias should be used to get :\n",
    "![title](8NN.JPG)\n",
    "\n",
    "Suppose O1 applies sigmoid on this input and gives the output as y_predicted.\n",
    "\n",
    "The sigmoid function applied is called the activation of this perceptron. It can be replaced by any other function like tanh, relu, leaky relu, or even identity function. \n",
    "\n",
    "Now simply using gradient descent to minimize error E wrt weight we do following process :\n",
    "\n",
    "![title](9NN.jpg)\n",
    "and it is given that O1 is sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](1NN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00172202],\n",
       "        [-0.00267029],\n",
       "        [ 0.00260616]]), array([[ 0.50065154],\n",
       "        [ 0.49998397],\n",
       "        [ 0.50022103],\n",
       "        [ 0.49955346]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    output0 = X  # is basically output of 0th layer i.e the input layer hence equals to X \n",
    "    output1 = sig(np.dot(output0, weights))    # as mentioned above the output  of O1 is sigmoid applied on z which is \n",
    "                                               # dot product of input and weight matrices \n",
    "    first_term = output1 - Y                   # basically y_pred - y_act \n",
    "    second_term = derivativeSig(output1) # output of unit O1 as mentioned above \n",
    "#     print(first_term.shape)\n",
    "    first_two = first_term * second_term\n",
    "#     print(first_two.shape)\n",
    "    changes = np.dot(output0.T, first_two)\n",
    "#     print(changes.shape)\n",
    "    weights = weights - learning_rate*changes # updating weights  \n",
    "    \n",
    "output1 = sig(np.dot(X, weights))\n",
    "weights,output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since above network had only one layer it wasn't able to create non-linear decision boundary and hence the results were poor.\n",
    "Now we will add one more layer and see if output changes or not. \n",
    "( You should test the above result by changing Y=[0,0,0,1] which is having a linear decision boundary. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights0 = 2* np.random.random((3,4)) -  1\n",
    "weights1 = 2* np.random.random((4, 1)) - 1\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(5000):\n",
    "    layer0 = X            # Input layer \n",
    "    layer1 = sig(np.dot(layer0, weights0))  # output  of layer1 is sigmoid applied on z1 i.e. input of layer 1\n",
    "    layer2 = sig(np.dot(layer1, weights1))  # output  of layer2 is sigmoid applied on z2 i.e. input of layer 2\n",
    "    \n",
    "    l2_error = layer2 - Y                   \n",
    "    l2_delta = l2_error * derivativeSig(layer2)   # delta k \n",
    "    net_change2 = np.dot(layer1.T, l2_delta)\n",
    "\n",
    "    l1_error = np.dot(l2_delta,weights1.T)           # error j \n",
    "    l1_delta = l1_error  * derivativeSig(layer1)  # delta j\n",
    "    net_change1 = np.dot(layer0.T, l1_delta)\n",
    "\n",
    "    weights0 = weights0 - learning_rate*net_change1\n",
    "    weights1 = weights1 - learning_rate*net_change2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01449743],\n",
       "       [ 0.97871508],\n",
       "       [ 0.97670817],\n",
       "       [ 0.02635731]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0 = X\n",
    "layer1 = sig(np.dot(layer0, weights0))\n",
    "layer2 = sig(np.dot(layer1, weights1))\n",
    "layer2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
